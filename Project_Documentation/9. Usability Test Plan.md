<h1>Game Shack: Solution to Board Game Collection Management</h1>
<h2>Usability Test Plan</h2>

## 1. Document Overview <a name="testplan"></a>
This document states the test plan for conducting a usability test during the development of our solution named 'Game Shack'. The goals of this testing include establishing a baseline of user performance, establishing and validating user performance measures, and identifying potential design concerns to be addressed in order to improve the efficiency, productivity, and end-user satisfaction of our prototype. 

The usability test objectives are stated below:

- To determine design inconsistencies and usability problem areas within the user interface and content areas of our prototype. Below are some potential problems that we are expecting to find as a result of this:
  - Navigation errors – failure to locate functions (such as adding a comment, giving ratings, liking a game), failure to follow recommended screen flow(confusion that might arise while navigating across the web pages in 'Game Shack').
  - Presentation errors – difficulty in interpreting the data present on the screens, expectations of the page layouts and information displayed must look convincing.
  - Control usage problems – entry field usage(handling the useer inputs that are invalid or not appropriate).

- Exercise the application or web site under controlled test conditions with representative users. Data will be used to access whether usability goals regarding an effective, efficient, and well-received user interface have been achieved.

- Establish baseline user performance and user-satisfaction levels of the UI experience for future usability evaluations.

The website is currently a prototype and can be considered as a beta version. The team took responsibility to contact few users to test this website online. We have conducted the usability test on various age groups such as children, teenagers and adults. This test took place on screen-sharing sessions using applications such as zoom, google meet or teams. There were a total of 20 participants to whom the purpose of this website was explained before launching the test and noting down the results. 

## 2. Executive Summary 

The purpose of the usability test on our website 'Game Shack' was to check how easily the user could navigate and understand the its functionality. The main goal was to ensure that the user had clarity on what he wanted to achieve while using this site and that they were satisfied with the UI experience. 

Upon review of this usability test plan, it is expected to document the user reviews and analysing the features to be developed in the future to lead the prototype in the right direction.

## 3. Methodology

The prototype is currently readily available to all the 4 members of the team who worked on it. First, the team leader has laid out the procedure to conduct the usability test and the metrics to be recorded per person. There were a total of 20 users of different age groups who took part in this test. Each team member handled 5 users for the sessions by providing support wherever necessary. This test took place on online screen sharing platforms such as zoom, teams, etc. We have instructed the users to perform few pre-requisite steps that will be discussed further in order to access our website online. One of the main tasks of this activity was to record the feedback of the users at various points of the test (login/logout process, navigation between pages, incorrect search values). Using these results we planned to develop a constructive future plan to enhance the prototype as well as to evaluate and improve the development as much as possible. 

### 3.1 Participants 
Since each team member is taking the responsibility of being a facilitator, trainer, data logger and observers, they have currently gathered 20 users in total for the usability test on 'Game Shack' website. It was important to include participants of diverse ages so that we could get broader feedbacks on the website on various categories. Since majority of the gaming markets consist of young users, 70% of our users were under 25 years (children, teenagers and early twenties) while the rest were of an older group of ages more than 40 (mainly parents' generation). We used this approach to get the views of both the categories and consider their experience as a learning step in developing a much more user-friendly website in the future. 

The participants' responsibilities will be to attempt to complete a set of representative task scenarios presented to them in as efficient and timely a manner as possible, and to provide feedback regarding the usability and acceptability of the user interface.

Before choosing our participants, we wanted to ensure that they had experience with online shopping before on apps like amazon, ebay, etc. We also had to include their language and computer usage experience before letting them take on the test. We kept in mind to carefully consider few users who were unable to catch on with the purpose of this website or who didn't have any interest in online e-commerce shopping or gaming to make the results of the test more useful. This was done to capture the experience of people who had no idea about the website and observe what they have done to explore in the website. 

### 3.2 Training

The participants will receive an overview of the usability test procedure, equipment and software before they begin. Since our website was in development phase, we had to ensure that their systems had python software available. They were given the steps provided in the READMe.md file before proceeding with the usability test. It was mandatory for the users to have any of the platforms like zoom, Google meet or microsoft teams for conducting the usability test via screen sharing sessions. This was for recording their comments, observing their interaction with the website and addressing any queries raised by them. A strong internet connection was needed to ensure that the flow of the website wasn't interrupted. There were cases of low speed internet and performance was tracked in those circumstances as well. 

### 3.3 Procedure

Participants will take part in the usability test via remote screen-sharing technology at convenience of their homes or offices. Verbal communication was provided during the sessions on call. 
Initially, the facillitator will take responsibility of guiding the users to setup and access the website on their systems. Once this is done, they inform the users on what tasks are needed to be performed and clarify all the user queries.

Sessions will begin when all participant questions are answered by the facilitator. The facilitator will inform the participant that time-on-task will be measured and that exploratory behavior outside the task flow should not occur until after task completion. This means that, once the users are given a list of tasks, the time taken by each of them to finish these tasks in sequence will be recorded first. After that is completed, the users are given few minutes to explore the website on their own accord and the test observers will monitor them.

### 3.4 Roles 
The roles involved in a usability test are as follows. An individual may play multiple roles and tests may not require all roles. In our case, we are only 4 team members and we all took the resposibility of being a trainer, facilitator, data logger and test observers. The job and responsibilities of these roles are described below:

#### 3.4.1 Trainer
   - Provide training overview prior to usability testing. This was done by the team leader to explain the usability tasks to the team members prior to the start of the test. 

#### 3.4.2 Facilitator
  - Provides overview of study to participants. 
  - Defines usability and purpose of usability testing to participants
  - Assists in conduct of participant and observer debriefing sessions
  - Responds to participant's requests for assistance

#### 3.4.3 Data Logger
 - Records participant’s actions and comments

#### 3.4.4 Test Observers
  - Silent observer who can access the meeting rooms when other sessions are on-going. 
  - Assists the data logger in identifying problems, concerns, coding bugs, and procedural errors.
  - Serve as note takers.

#### 3.4.5 Test Participants
- Users who will perform the usability tasks of this test
- Give consent to the test organisers to use their feedback for future developments
- Provide reviews and comments on the prototype
- Explore the website on their own risks and provide constructive criticism

## 4. Usability Tasks <a name="tasks"></a>

This section describes the usability tasks that need to be performed by the test participants during the sessions. The team leader will provide these list of tasks to the facilitators, who will in return, explain them to the test takers and emphasize on the importance of completing them sequentially. The feedback and performance are measured either qualitatively or quantitatively, depending on the metric, based on the reports of these tasks and later used for project analysis.

This is a crucial part of the document as most of the calculations and logging are based on the performance of completing below tasks: 
#### 4.1 Tasks for the Facilitators/Data Loggers:
1. Organise a session with the test takers on screen sharing platforms
2. Ensure that the users have done the pre-requisite steps
3. Check the users have necessary softwares to open the website
4. Inform the users of the usability test activities
5. Clarify all the doubts before starting the test 
6. Guide the users through-out the session. 
7. Log user performances and reviews during and after the session.

#### 4.2 Pre-requisite Steps <a name="prereq"></a>
Below are few necessary steps that need to be performed before beginning a test by the user: 
1. Ensure that they give complete attention during the test 
2. Try their best to connect to internet that doesn't interrupt
3. Have basic experience with using e-commerce websites. (Not mandatory but preferred)
4. Ensure a proper web-browser is present to access the website
5. Must have necessary software to run few commands to access a website. (Facilitator to help with this step)
6. Clarify all doubts and questions before beginning the test

#### 4.3 Tasks for the Users: <a name="usertask"></a>
1. Registering to the website / Logging in to the website
2. Navigate across the home screen and state their observations
3. Use the search tool for games of their choice
4. View the game details in a new page
5. See the components of the game
6. View the game rules and it's description
7. Add a game to the Library/Wishlist
8. Attempt to purchase a game
9. Check the recommendations
10. Read or provide reviews and ratings to a game
11. Join game commmunities of their choice
12. Look at available clubs and join any if interested
13. Check user profile and summary
14. Logging out of the website
15. Explore the website for few minutes and give feedback to the Data loggers.


## 5. Usability Metrics
Usability metrics refers to user performance measured against specific performance goals necessary to satisfy usability requirements. Task/Scenario completion is the first metric to be considered. Any errors or unexpected behaviours must also be logged.  Time-to-completion of scenarios will also be collected.

#### 5.1 Scenario Completion

Each scenario will require, or request, that the participant obtains or inputs specific data that would be used in course of the tasks mentioned above.  The scenario/task is completed when the participant indicates the scenario's goal has been obtained (whether successfully or unsuccessfully) or the participant requests and receives sufficient guidance to compplete it till the end. In our case, these scenarios include the tasks that have been provided to the users in the usability tasks section above. 

#### 5.2 Critical Errors

In general, critical errors are unresolved errors during the process of completing the task or errors that produce an incorrect outcome. They are deviations at completion from the targets of the scenario.  Obtaining or otherwise reporting of the wrong data value due to participant workflow is a critical error. Participants may or may not be aware that the task goal is incorrect or incomplete.

#### 5.3 Non-critical Errors

Non-critical errors are errors that are recovered from by the participant or, if not detected, do not result in processing problems or unexpected results.  Although non-critical errors can be undetected by the participant, when they are detected they are generally frustrating to the participant.

Noncritical errors can always be recovered from during the process of completing the scenario.  Exploratory behavior, such as opening the wrong menu while searching for a function, will be coded as a non-critical error. 

## 6. Usability Goals

The next section describes the usability goals for 'Game Shack' website prototype. 

### 6.1 Completion Rate

Completion rate is the percentage of test participants who successfully complete the task without critical errors.  A critical error is defined as an error that results in an incorrect or incomplete outcome. The goal of the team is to achieve 100% in completion rate, this indicates that the users find no difficulty in finishing all their tasks and can even explore the website with complete understanding. 

### 6.2 Error-free rate

Error-free rate is the percentage of test participants who complete the task without any errors (critical **or** non-critical errors).  This will be calculated based on the frequency of failure to achieve a task in general or the most difficult task to the users during the explorative phase. 
A non-critical error is an error that would not have an impact on the final output of the task but would result in the task being completed less efficiently. This includes discrepancy in the recommender system due to the user not entering their preferred genres during the registration. 
Since the website is a prototype and is currently in its initial phase before launching, we will be expecting atleast 70% of error-free rate after overall analysis. 

### 6.3 Subjective Measures

Subjective opinions about specific tasks, time to perform each task, features, and functionality will be surveyed.  At the end of the test, participants will rate their satisfaction with the overall system.  Combined with the interview/debriefing session, these data are used to assess attitudes of the participants.

## 7. Problem Severity 

To prioritize recommendations, a method of problem severity classification will be used in the analysis of the data collected during evaluation activities.  The approach treats problem severity as a combination of two factors - the impact of the problem and the frequency of users experiencing the problem during the evaluation.

### 7.1 Impact

Impact is the ranking of the consequences of the problem by defining the level of impact that the problem has on successful task completion.  There are three levels of impact:

- High - prevents the user from completing the task (critical error)
- Moderate - causes user difficulty but the task can be completed (non-critical error)
- Low - minor problems that do not significantly affect the task completion (non-critical error)

### 7.2 Frequency

Frequency is the percentage of participants who experience the problem when working on a task.

- High: 30% or more of the participants experience the problem
- Moderate: 11% - 29% of participants experience the problem
- Low: 10% or fewer of the participants experience the problem

### 7.3 Problem Severity Classification

The identified severity for each problem implies a general reward for resolving it, and a general risk for not addressing it, in the current release.

**Severity 1** - High impact problems that often prevent a user from correctly completing a task.  They occur in varying frequency and are characteristic of calls to the Help Desk.  Reward for resolution is typically exhibited in fewer Help Desk calls and reduced redevelopment costs.

**Severity 2** - Moderate to high frequency problems with moderate to low impact are typical of erroneous actions that the participant recognizes needs to be undone.  Reward for resolution is typically exhibited in reduced time on task and decreased training costs.

**Severity 3** - Either moderate problems with low frequency or low problems with moderate frequency; these are minor annoyance problems faced by a number of participants.  Reward for resolution is typically exhibited in reduced time on task and increased data integrity.

**Severity 4** - Low impact problems faced by few participants; there is low risk to not resolving these problems. Reward for resolution is typically exhibited in increased user satisfaction.

## 8. Reporting Results
The [Usability Report and Analysis](10. Usability Report And Analysis.md#usability) will be provided at the conclusion of the usability test.  It will consist of a report that will present and analyse the results, evaluate the usability metrics against the pre-approved goals, subjective evaluations, and specific usability problems and recommendations for resolution. The recommendations will be categorically sized by development to aid in implementation strategy. 
